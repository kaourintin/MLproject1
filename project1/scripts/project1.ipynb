{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "#Pensez Ã  retirer 'fermeli' du path\n",
    "DATA_TRAIN_PATH = '/home/ML_course/projects/project1/data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "def clean_data(tX):\n",
    "    #replace -999 by the mean of the valid features of the column\n",
    "    tX[tX == -999] = np.NaN\n",
    "    mean = np.nanmean(tX,axis=0)\n",
    "    inds = np.where(np.isnan(tX))    \n",
    "    tX[inds]= np.take(mean, inds[1])\n",
    "    \n",
    "    #standardize features \n",
    "    std= np.std(tX,axis=0)\n",
    "    newMean = np.nanmean(tX,axis=0)\n",
    "    \n",
    "    return (tX-newMean)/std\n",
    "\n",
    "tX = clean_data(tX)\n",
    "print(\"finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validation_visualization(lambds, mse_tr, mse_te):\n",
    "    print(\"inside the vizualisation function\")\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.xlim(1e-4, 1)\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"cross_validation\")\n",
    "\n",
    "\n",
    "def bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te):\n",
    "    \"\"\"visualize the bias variance decomposition.\"\"\"\n",
    "    rmse_tr_mean = np.expand_dims(np.mean(rmse_tr, axis=0), axis=0)\n",
    "    rmse_te_mean = np.expand_dims(np.mean(rmse_te, axis=0), axis=0)\n",
    "    plt.plot(\n",
    "        degrees,\n",
    "        rmse_tr.T,\n",
    "        linestyle=\"-\",\n",
    "        color=([0.7, 0.7, 1]),\n",
    "        linewidth=0.3)\n",
    "    plt.plot(\n",
    "        degrees,\n",
    "        rmse_te.T,\n",
    "        linestyle=\"-\",\n",
    "        color=[1, 0.7, 0.7],\n",
    "        linewidth=0.3)\n",
    "    plt.plot(\n",
    "        degrees,\n",
    "        rmse_tr_mean.T,\n",
    "        'b',\n",
    "        linestyle=\"-\",\n",
    "        label='train',\n",
    "        linewidth=3)\n",
    "    plt.plot(\n",
    "        degrees,\n",
    "        rmse_te_mean.T,\n",
    "        'r',\n",
    "        linestyle=\"-\",\n",
    "        label='test',\n",
    "        linewidth=3)\n",
    "    plt.xlim(1, 9)\n",
    "    plt.ylim(0.2, 0.7)\n",
    "    plt.xlabel(\"degree\")\n",
    "    plt.ylabel(\"error\")\n",
    "    plt.legend(loc=1)\n",
    "    plt.title(\"Bias-Variance Decomposition\")\n",
    "    plt.savefig(\"bias_variance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just before the visualization function call\n",
      "[0.3900137598433009, 0.3900137422626319, 0.390013743827957, 0.39001379419807153, 0.39001395358749874, 0.3900143406973011, 0.3900151845774912, 0.3900169196383554, 0.3900203569342425, 0.39002698674662506, 0.39003949905046775, 0.39006264690440007, 0.3901046076883091, 0.3901789758148999, 0.39030736410584466, 0.39052218843071357, 0.3908685244740736, 0.39140321974730274, 0.392189482830432, 0.3932869159033533, 0.3947403770646348, 0.39657401710373963, 0.39879620038195, 0.40141595195470353, 0.4044648664623516, 0.40801379105792, 0.4121730318876389, 0.4170689254310312, 0.4227983791569605, 0.42937468068450485]\n",
      "[0.3899036678172342, 0.38990371747066044, 0.3899038109298132, 0.3899039866642678, 0.38990431665085057, 0.3899049351330188, 0.3899060914182595, 0.3899082458428653, 0.3899122418534706, 0.3899196089454732, 0.3899330826369471, 0.3899574660269559, 0.39000098736801125, 0.39007728693851373, 0.39020801052839293, 0.39042558532290295, 0.3907750700483024, 0.391313267256325, 0.39210332414225957, 0.393204788445666, 0.3946625006642939, 0.3965006569303362, 0.39872773561005803, 0.40135290667244894, 0.404407848709865, 0.40796332335631175, 0.41212934125182765, 0.4170318189715178, 0.4227672871700427, 0.4293488312946844]\n",
      "inside the vizualisation function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print(\"starting log reg\")\\n_,w = np.squeeze(logistic_regression(y, tX, np.zeros((tX.shape[1], 1)), 10000, 0.01))\\nlabels = classify_with_threshold(0.5,tX,w)\\nprint(accuracy(y,labels))\\n#print(len(tX))\\nprint(\"ending log reg\")'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_mse(e):\n",
    "   return 1/2*np.mean(e**2)\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "\n",
    "\n",
    "############################### Linear regression with gradient descent################################   \n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    y= y.reshape(y.shape[0],1)\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad, err = compute_gradient(y, tx, w)\n",
    "        loss = calculate_mse(err)\n",
    "        w = w - gamma * grad\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}, w1={w1}, w1={w1}\".format(\n",
    "        bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws\n",
    "#least_squares_GD(y, tX, np.full((30,1),0.00001), 100, 0.001)\n",
    "\n",
    "\n",
    "######## least squares ##################################################################################\n",
    "def least_square(y,tx):\n",
    "    s= tx.T.dot(tx)\n",
    "    t = tx.T.dot(y)\n",
    "    return np.linalg.solve(s, t)\n",
    "\n",
    "\n",
    "################################# Linear regression with SGD ###########################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    y= y.reshape(y.shape[0],1)\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_gradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            e = y - tx.dot(w)\n",
    "            loss = calculate_mse(e)\n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w,max_iters, gamma):\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient descent.\n",
    "    # ***************************************************\n",
    "    batch_size = 1\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        \n",
    "        B = tx[np.random.choice(batch_size):]\n",
    "        \n",
    "        grad = compute_gradient(y,B,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * grad\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    return losses, ws\n",
    "\n",
    "#stochastic_gradient_descent(y, tX, np.full((30,1),0.00001),4, 100, 0.00001)\n",
    "\n",
    "################################## Ridge regression ##############################################\n",
    "\n",
    "\n",
    "def ridge_regression_solve(y, tx, lambda_):\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    return np.linalg.solve(a, b)\n",
    "\n",
    "def ridge_regression(y, tx, lambda_ ):\n",
    "    y= y.reshape(y.shape[0],1)\n",
    "    w = ridge_regression_solve(y, tx, lambda_)\n",
    "    err = y - tx.dot(w)\n",
    "    rmse = np.sqrt(2 * calculate_mse(err))\n",
    "    return rmse, w\n",
    "    \n",
    "\n",
    "        \n",
    "#lambdas = np.logspace(-5, 0, 15)\n",
    "#for ind, lambda_ in enumerate(lambdas):\n",
    " #   ridge_regression(y, tX,lambda_)\n",
    "\n",
    "    \n",
    "    \n",
    "################################## Logisitic regression ##################################\n",
    "\n",
    "def accuracy(labels_gt, labels_pred):\n",
    "    \"\"\" Computes accuracy.\n",
    "    \n",
    "    Args:\n",
    "        labels_gt (np.array): GT labels of shape (N, ).\n",
    "        labels_pred (np.array): Predicted labels of shape (N, ).\n",
    "        \n",
    "    Returns:\n",
    "        float: Accuracy, in range [0, 1].\n",
    "    \"\"\"\n",
    "    return (len(labels_gt) - np.sum(np.abs(labels_pred - labels_gt)))/len(labels_gt)\n",
    "\n",
    "def sigmoid(t):\n",
    "    t[t<-10] = -10\n",
    "    sigmoid = 1/(1 +np.exp(-t)) \n",
    "    sigmoid[sigmoid == 1] = 0.9999\n",
    "    return sigmoid\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \n",
    "    sig = sigmoid(tx.dot(w))\n",
    "    \n",
    "    cost =  - (1-y).T.dot(np.log(1-sig)) - y.T.dot(np.log(sig))\n",
    "    return cost\n",
    "    \n",
    "\n",
    "def calculate_gradient_LR(y, tx, w):\n",
    "    \n",
    "    return tx.T.dot(sigmoid(tx.dot(w))- y)\n",
    "                               \n",
    "    \n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient_LR(y,tx,w)\n",
    "    w = w- gamma * grad\n",
    "    \n",
    "    return loss, w\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    y[y == -1] = 0 #because we have to have values between 0 and 1\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    y= y.reshape(y.shape[0],1)\n",
    "    w = initial_w\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        #print(loss)\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "            \n",
    "    return loss, w\n",
    "\n",
    "def classify_with_threshold(threshold,tX,w):\n",
    "    predictions = tX @ w\n",
    "    labels = []\n",
    "    for pred in predictions:\n",
    "        labels.append(0 if pred < 0.5 else 1)\n",
    "        \n",
    "    return labels\n",
    "\n",
    "################################## Logisitic regression with REgula##################################\n",
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    num_samples = y.shape[0]\n",
    "    \n",
    "    loss = calculate_loss(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "    gradient = calculate_gradient_LR(y, tx, w) + 2 * lambda_ * w\n",
    "    return loss, gradient\n",
    "\n",
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "   \n",
    "    loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w -= gamma * gradient\n",
    "    return loss, w\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_ , initial_w, max_iters, gamma):\n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    y = y.reshape(y.shape[0],1)\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return losses,w \n",
    "\n",
    "#reg_logistic_regression(y, tX, 0.1, np.zeros((tX.shape[1], 1)), 10000, 0.01)\n",
    "\n",
    "################################## Functions to optimize the model ##################################\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data based on the given ratio: \n",
    "    # ***************************************************\n",
    "    indices = np.arange(len(y))\n",
    "    training_indices = np.random.choice(indices, (int(ratio * len(y))))\n",
    "    test_indices = np.setdiff1d(indices,training_indices)\n",
    "    training_x = np.take(x,training_indices)\n",
    "    training_y = np.take(y,training_indices)\n",
    "    test_x = np.take(x,test_indices)\n",
    "    test_y = np.take(y,test_indices)\n",
    "    \n",
    "    \n",
    "    return training_x,test_x,training_y,test_y\n",
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # polynomial basis function: TODO\n",
    "    # this function should return the matrix formed\n",
    "    # by applying the polynomial basis to the input data\n",
    "    # ***************************************************\n",
    "    result = np.zeros((len(x), degree))\n",
    "    for i in range(len(x)):\n",
    "        for j in range(degree):\n",
    "            result[i][j] = x[i]**j\n",
    "    return result\n",
    "\n",
    "\n",
    "########\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    #split the arrays using the k_indices matrix : \n",
    "    train_indices = k_indices[~(np.arange(k_indices.shape[0]) == k)].reshape(-1)\n",
    "    test_indices = k_indices[k]\n",
    "    \n",
    "    x_train = x[train_indices]\n",
    "    x_test = x[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    #augment data :\n",
    "    x_train_poly = x_train #build_poly(x_train, degree)\n",
    "    x_test_poly = x_test #build_poly(x_test, degree)\n",
    "    # ridge :\n",
    "    rmse,w = ridge_regression(y_train, x_train_poly, lambda_)\n",
    "    # calc loss:\n",
    "    training_loss = calculate_mse(y_train - np.squeeze(x_train_poly.dot(w)))\n",
    "    testing_loss = calculate_mse(y_test - np.squeeze(x_test_poly.dot(w)))\n",
    "    return training_loss, testing_loss,w\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cross_validation_k_lambda_run(y, x, w):\n",
    "    seed = 1\n",
    "    degree = 0\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    \n",
    "    training_losses = []\n",
    "    test_losses = []\n",
    "    #split the data\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "   \n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        k_losses_train = []\n",
    "        k_losses_test = []\n",
    "        for k in range(k_fold):\n",
    "            k_loss_train, k_loss_test, _ = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "            k_losses_train.append(k_loss_train)\n",
    "            k_losses_test.append(k_loss_test)\n",
    "            \n",
    "        training_losses.append(np.mean(k_losses_train))\n",
    "        test_losses.append(np.mean(k_losses_test))\n",
    "    print(\"just before the visualization function call\")\n",
    "    print(test_losses)\n",
    "    print(training_losses)\n",
    "    cross_validation_visualization(lambdas, training_losses, test_losses)\n",
    "    \n",
    "    \n",
    "\n",
    "cross_validation_k_lambda_run(y, tX, np.zeros((tX.shape[1], 1)))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "'''print(\"starting log reg\")\n",
    "_,w = np.squeeze(logistic_regression(y, tX, np.zeros((tX.shape[1], 1)), 10000, 0.01))\n",
    "labels = classify_with_threshold(0.5,tX,w)\n",
    "print(accuracy(y,labels))\n",
    "#print(len(tX))\n",
    "print(\"ending log reg\")'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "2f46dc9274ee3932e0f108b5497f3d5e8b675a9cadd6890a524ec505a688e6fd"
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
